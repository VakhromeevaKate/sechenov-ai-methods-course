{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKVEZj3Ze+I0RES/DMeB62",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VakhromeevaKate/sechenov-ai-methods-course/blob/main/PracticeAIMethods_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Промт-инжиниринг: Mistral, Qwen, DeepSeek, GigaChat, Humata, multillm.ai. chathub.gg\n",
        "\n",
        "Определение: это процесс создания и оптимизации текстовых запросов (промптов), которые используются для взаимодействия с большими языковыми моделями (LLM) искусственного интеллекта.\n",
        "\n",
        "Цель промпт-инжиниринга — получить от ИИ-модели максимально точный, релевантный и полезный ответ, понимая её возможности и ограничения. Промпт-инженеры формулируют инструкции и задают вопросы так, чтобы направить модель к выполнению конкретной задачи, будь то генерация текста, написание кода, перевод или что-то ещё.\n",
        "\n",
        "С чем будем работать на семинаре:\n",
        "- Mistral (https://chat.mistral.ai/chat?q=),\n",
        "- Qwen (https://chat.qwen.ai/),\n",
        "- DeepSeek (https://chat.deepseek.com/),\n",
        "- GigaChat (https://giga.chat/?error=sso_error),\n",
        "- Humata (https://www.humata.ai/),\n",
        "- multillm.ai (https://multillm.ai).\n",
        "- chathub.gg (https://app.chathub.gg/?utm_source=chathub.gg)"
      ],
      "metadata": {
        "id": "x48QHW7Tgadp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Что GPT-модели умеют:\n",
        "- искать информацию (поиск)\n",
        "- обобщать информацию\n",
        "- генерировать информацию (иозбражения, текст и т п)\n",
        "- стилизация (в основном, за счет контекста, напеример: ты - школьник 5 класса средней школы. Напиши эссе по теме \"Как я провел лето в деревне у бабушки\")"
      ],
      "metadata": {
        "id": "_PF89sik_uFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Структура промпта:\n",
        "Контекст: ты - учитель, ребенок, профессионал в какой-то области...\n",
        "Задача: напиши текст, сгенерируй изображение, напиши код на языке lisp...\n",
        "Формат ответа: в виде таблицы, текста, списка... или пример того, как должен выглядеть ответ.\n",
        "\n",
        "Задача для GPT должна быть сформулирована полностью и ясно.\n",
        "\n",
        "Поисковые промпты могут выглядеть просто как вопрос - сколько спутников у Сатурна? Кто автор Тихий Дон? и т п\n",
        "\n"
      ],
      "metadata": {
        "id": "hG7TYcUqA7oO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Что такое галлюцинации в LLM?\n",
        "\n",
        "**Галлюцинации ИИ** — это явление, при котором большая языковая модель (LLM) генерирует информацию, которая является:\n",
        "\n",
        "1.  **Неверной** или **искаженной**: Модель утверждает факты, не соответствующие действительности (например, неправильные даты, имена, события).\n",
        "2.  **Несуществующей** или **выдуманной**: Модель полностью сочиняет информацию, включая несуществующие книги, статьи, законы, исторические события или даже цитаты.\n",
        "3.  **Не относящейся к контексту** или **бессмысленной**: Ответ формально правильный, но не имеет отношения к заданному вопросу или является абсурдным в данном контексте.\n",
        "\n",
        "Проще говоря, **это когда модель с абсолютной уверенностью говорит неправду**.\n",
        "\n",
        "**Примеры галлюцинаций:**\n",
        "\n",
        "*   **Фактическая ошибка:** \"Барак Обама был президентом США в 2005 году.\" (На самом деле он вступил в должность в 2009).\n",
        "*   **Вымысел:** \"В своей нобелевской речи Альберт Эйнштейн сказал: 'Воображение важнее знаний, но знания указывают дорогу'.\" (Эйнштейн никогда не получал Нобелевскую премию за теорию относительности, а его известная фраза звучит иначе).\n",
        "*   **\"Галлюцинация\" источников:** Модель может приписать реальной статье несуществующие данные или сослаться на научную работу, которой не существует, но с убедительным названием и якобы реальными авторами.\n",
        "\n",
        "---\n",
        "\n",
        "### Почему они возникают?\n",
        "\n",
        "Галлюцинации — это не \"баг\" или ошибка в коде, а фундаментальное следствие того, **как работают LLM**. Это системная особенность их архитектуры.\n",
        "\n",
        "Вот основные причины:\n",
        "\n",
        "#### 1. Статистическое прогнозирование, а не поиск истины\n",
        "**Самая главная причина.** LLM не \"думают\" и не \"понимают\" мир как люди. Они — сложнейшие системы **статистического прогнозирования**. Их задача — предугадать следующее самое вероятное слово (токен) в последовательности на основе огромного массива данных, на которых они обучались.\n",
        "\n",
        "Они оптимизированы для создания **правдоподобного и грамматически корректного текста**, а не для проверки фактов. Модель выбирает слово, которое *статистически чаще всего* встречалось в похожем контексте в её обучающих данных, даже если это приводит к factual ошибке.\n",
        "\n",
        "#### 2. Ограничения обучающих данных\n",
        "*   **Неполнота данных:** Модель обучается на срезах интернета, который полон неточностей, мифов, устаревшей информации и откровенной лжи. Модель усваивает всё это без разбора.\n",
        "*   **Противоречия в данных:** В данных могут содержаться противоположные утверждения по одному и тому же вопросу. Модель может \"выбрать\" не то, которое является истинным, а то, которое статистически более вероятно в контексте запроса.\n",
        "\n",
        "#### 3. Отсутствие модели мира\n",
        "У LLM нет внутренней модели реального мира, основанной на причинно-следственных связях и опыте. Они не могут отличить возможное от невозможного, если оба варианта часто встречаются в текстах. Они не \"знают\", что человек не может присутствовать на двух континентах одновременно, если в данных много примеров художественной литературы, где такое происходит.\n",
        "\n",
        "#### 4. Чрезмерная уверенность и \"угодливость\" (SycoPhancy)\n",
        "LLM часто разрабатываются и настраиваются (через RLHF — обучение с подкреплением на основе человеческих предпочтений) для того, чтобы давать уверенные, прямые и полезные ответы. Это может заставить модель **придумать ответ**, если она \"считает\", что так будет лучше соответствовать запросу пользователя, даже если у неё нет точной информации. Она скорее сочинит что-то правдоподобное, чем скажет \"я не знаю\".\n",
        "\n",
        "#### 5. Чувствительность к формулировкам (Prompt Sensitivity)\n",
        "Один и тот же вопрос, заданный разными словами, может привести к совершенно разным ответам — как правильным, так и галлюцинированным. Неудачно составленный промт (запрос) может легко направить модель по ложному пути.\n",
        "\n",
        "### Что с этим делают?\n",
        "\n",
        "С галлюцинациями активно борются, но полностью искоренить их пока невозможно. Основные методы:\n",
        "\n",
        "1.  **Поиск по внешним базам знаний (RAG — Retrieval-Augmented Generation):** Это самый эффективный на сегодня подход. Прежде чем генерировать ответ, модель *ищет информацию в надёжных внешних источниках* (базы данных, свежие статьи, официальные сайты) и использует эти актуальные данные для формирования ответа. Это сильно снижает количество вымысла.\n",
        "2.  **Тонкая настройка и обучение с подкреплением (RLHF):** Моделей дополнительно обучают на наборах данных, где помечены правильные и неправильные ответы, поощряя их быть более точными и осторожными.\n",
        "3.  **Промт-инжиниринг:** Правильная формулировка запросов, например, добавление инструкций: \"Если ты не уверен, скажи 'я не знаю'\", или \"Ответь строго на основе приведённого ниже текста\".\n",
        "4.  **Прозрачность:** Ответы моделей всё чаще снабжаются ссылками на источники, чтобы пользователь мог проверить информацию.\n",
        "\n",
        "### Вывод\n",
        "\n",
        "**Галлюцинации — это неизбежный побочный продукт статистической природы языковых моделей.** Они возникают потому, что модель создаёт текст, а не извлекает факты. Поэтому **критическое мышление и проверка информации из авторитетных источников остаются абсолютно необходимыми** при использовании любого ИИ-ассистента. Всегда воспринимайте их ответ как *правдоподобный черновик*, а не как истину в последней инстанции.\n"
      ],
      "metadata": {
        "id": "tEaINEe4eAsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание:\n",
        "- Взять 5 нейросетей (можно из списка, можно свои)\n",
        "- Сгенерировать с их помощью код на python, который выводит с помощью matplotlib любую картинку\n",
        "- запустить код\n",
        "- прислать полученные результаты и свой вывод (какая для каких задач лучше)"
      ],
      "metadata": {
        "id": "qDop7v2MhkVL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tKjlKPvT_tGX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}